[02/06 19:41:35] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 19:41:36] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 19:41:36] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 19:41:36] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 19:41:36] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 19:41:36] d2.utils.env INFO: Using a generated random seed 38087951
[02/06 19:44:11] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 19:44:12] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 19:44:12] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 19:44:12] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 19:44:12] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 19:44:12] d2.utils.env INFO: Using a generated random seed 13893549
[02/06 19:44:45] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 19:44:45] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 19:44:53] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 7.81 seconds.
[02/06 19:44:53] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 19:44:56] d2.data.build INFO: Removed 0 images with no usable annotations. 8038 images left.
[02/06 19:44:56] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    fish    | 17238        |  reptiles  | 290          | arthropoda | 334          |
|   corals   | 9245         |  mollusk   | 1832         |   plants   | 1398         |
|   ruins    | 670          |  garbage   | 630          |   human    | 1511         |
|   robots   | 414          |            |              |            |              |
|   total    | 33562        |            |              |            |              |[0m
[02/06 19:44:56] d2.data.build INFO: Using training sampler TrainingSampler
[02/06 19:44:56] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/06 19:44:56] d2.data.common INFO: Serializing 8038 elements to byte tensors and concatenating them all ...
[02/06 19:44:58] d2.data.common INFO: Serialized dataset takes 138.37 MiB
[02/06 19:44:58] d2.data.build INFO: Making batched data loader with batch_size=4
[02/06 19:44:59] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 19:45:08] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/hoangnv/.torch/iopath_cache/detectron2/ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 19:45:08] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/06 19:45:08] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[02/06 19:45:09] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.vit.blocks.0.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm2.{bias, weight}[0m
[34mbackbone.vit.neck.0.weight[0m
[34mbackbone.vit.neck.1.{bias, weight}[0m
[34mbackbone.vit.neck.2.weight[0m
[34mbackbone.vit.neck.3.{bias, weight}[0m
[34mbackbone.vit.patch_embed.proj.{bias, weight}[0m
[34mbackbone.vit.pos_embed[0m
[34mmask_decoder.iou_prediction_head.layers.0.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.1.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.2.{bias, weight}[0m
[34mmask_decoder.iou_token.weight[0m
[34mmask_decoder.mask_tokens.weight[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.2.{bias, weight}[0m
[34mmask_decoder.output_upscaling.0.{bias, weight}[0m
[34mmask_decoder.output_upscaling.1.{bias, weight}[0m
[34mmask_decoder.output_upscaling.3.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.norm_final_attn.{bias, weight}[0m
[34mpe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.mask_downscaling.0.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.1.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.3.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.4.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.6.{bias, weight}[0m
[34mprompt_encoder.no_mask_embed.weight[0m
[34mprompt_encoder.not_a_point_embed.weight[0m
[34mprompt_encoder.pe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.point_embeddings.0.weight[0m
[34mprompt_encoder.point_embeddings.1.weight[0m
[34mprompt_encoder.point_embeddings.2.weight[0m
[34mprompt_encoder.point_embeddings.3.weight[0m
[34mproposal_generator.box_head.conv.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.4.{bias, weight}[0m
[34mproposal_generator.channel_attention.fc.0.weight[0m
[34mproposal_generator.channel_attention.fc.2.weight[0m
[34mproposal_generator.cls_head.conv.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p2_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p3_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p4_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.1.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.1.{bias, weight}[0m
[34mproposal_generator.pe_layer.positional_encoding_gaussian_matrix[0m
[34mproposal_generator.prompt_head.conv.0.{bias, weight}[0m
[34mproposal_generator.prompt_head.conv.1.{bias, running_mean, running_var, weight}[0m
[34mproposal_generator.prompt_head.mlp.1.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.3.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.5.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.objectness_logits.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.0.norm1.{bias, weight}[0m
[34mtransformer.layers.0.norm2.{bias, weight}[0m
[34mtransformer.layers.0.norm3.{bias, weight}[0m
[34mtransformer.layers.0.norm4.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.1.norm1.{bias, weight}[0m
[34mtransformer.layers.1.norm2.{bias, weight}[0m
[34mtransformer.layers.1.norm3.{bias, weight}[0m
[34mtransformer.layers.1.norm4.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.norm_final_attn.{bias, weight}[0m
[02/06 19:45:09] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.{bias, weight}[0m
  [35mfc1000.{bias, weight}[0m
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.shortcut.weight[0m
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv1.weight[0m
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv2.weight[0m
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv3.weight[0m
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv1.weight[0m
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv2.weight[0m
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv3.weight[0m
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv1.weight[0m
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv2.weight[0m
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv3.weight[0m
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.shortcut.weight[0m
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv1.weight[0m
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv2.weight[0m
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv3.weight[0m
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv1.weight[0m
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv2.weight[0m
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv3.weight[0m
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv1.weight[0m
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv2.weight[0m
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv3.weight[0m
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv1.weight[0m
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv2.weight[0m
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv3.weight[0m
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.shortcut.weight[0m
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv1.weight[0m
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv2.weight[0m
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv3.weight[0m
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv1.weight[0m
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv2.weight[0m
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv3.weight[0m
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv1.weight[0m
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv2.weight[0m
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv3.weight[0m
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv1.weight[0m
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv2.weight[0m
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv3.weight[0m
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv1.weight[0m
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv2.weight[0m
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv3.weight[0m
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv1.weight[0m
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv2.weight[0m
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv3.weight[0m
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.shortcut.weight[0m
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv1.weight[0m
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv2.weight[0m
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv3.weight[0m
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv1.weight[0m
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv2.weight[0m
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv3.weight[0m
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv1.weight[0m
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv2.weight[0m
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv3.weight[0m
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}[0m
[02/06 19:45:09] d2.engine.train_loop INFO: Starting training from iteration 0
[02/06 19:45:16] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 152, in train
    self.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 307, in run_step
    loss_dict = self.model(data)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/uwsam.py", line 137, in forward
    proposals, proposal_losses, all_visual_prompts, pred_class_logits, pred_proposal_deltas = self.proposal_generator(images, features, gt_instances)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/eupg.py", line 293, in forward
    gt_as_prop.objectness_logits = torch.ones(len(gt), device=p.device) * 10.0
  File "/home/hoangnv/detectron2/detectron2/structures/instances.py", line 66, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'device' in the given Instances!
[02/06 19:45:16] d2.engine.hooks INFO: Total training time: 0:00:07 (0:00:00 on hooks)
[02/06 19:45:16] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 16302M
[02/06 19:47:23] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 19:47:24] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 19:47:24] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 19:47:24] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 19:47:24] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 19:47:24] d2.utils.env INFO: Using a generated random seed 26128650
[02/06 19:48:02] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 19:48:02] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 19:48:12] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 10.58 seconds.
[02/06 19:48:13] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 19:48:16] d2.data.build INFO: Removed 0 images with no usable annotations. 8038 images left.
[02/06 19:48:16] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    fish    | 17238        |  reptiles  | 290          | arthropoda | 334          |
|   corals   | 9245         |  mollusk   | 1832         |   plants   | 1398         |
|   ruins    | 670          |  garbage   | 630          |   human    | 1511         |
|   robots   | 414          |            |              |            |              |
|   total    | 33562        |            |              |            |              |[0m
[02/06 19:48:16] d2.data.build INFO: Using training sampler TrainingSampler
[02/06 19:48:16] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/06 19:48:16] d2.data.common INFO: Serializing 8038 elements to byte tensors and concatenating them all ...
[02/06 19:48:18] d2.data.common INFO: Serialized dataset takes 138.37 MiB
[02/06 19:48:18] d2.data.build INFO: Making batched data loader with batch_size=4
[02/06 19:48:19] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 19:48:19] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/hoangnv/.torch/iopath_cache/detectron2/ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 19:48:20] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/06 19:48:20] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[02/06 19:48:20] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.vit.blocks.0.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm2.{bias, weight}[0m
[34mbackbone.vit.neck.0.weight[0m
[34mbackbone.vit.neck.1.{bias, weight}[0m
[34mbackbone.vit.neck.2.weight[0m
[34mbackbone.vit.neck.3.{bias, weight}[0m
[34mbackbone.vit.patch_embed.proj.{bias, weight}[0m
[34mbackbone.vit.pos_embed[0m
[34mmask_decoder.iou_prediction_head.layers.0.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.1.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.2.{bias, weight}[0m
[34mmask_decoder.iou_token.weight[0m
[34mmask_decoder.mask_tokens.weight[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.2.{bias, weight}[0m
[34mmask_decoder.output_upscaling.0.{bias, weight}[0m
[34mmask_decoder.output_upscaling.1.{bias, weight}[0m
[34mmask_decoder.output_upscaling.3.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.norm_final_attn.{bias, weight}[0m
[34mpe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.mask_downscaling.0.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.1.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.3.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.4.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.6.{bias, weight}[0m
[34mprompt_encoder.no_mask_embed.weight[0m
[34mprompt_encoder.not_a_point_embed.weight[0m
[34mprompt_encoder.pe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.point_embeddings.0.weight[0m
[34mprompt_encoder.point_embeddings.1.weight[0m
[34mprompt_encoder.point_embeddings.2.weight[0m
[34mprompt_encoder.point_embeddings.3.weight[0m
[34mproposal_generator.box_head.conv.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.4.{bias, weight}[0m
[34mproposal_generator.channel_attention.fc.0.weight[0m
[34mproposal_generator.channel_attention.fc.2.weight[0m
[34mproposal_generator.cls_head.conv.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p2_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p3_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p4_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.1.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.1.{bias, weight}[0m
[34mproposal_generator.pe_layer.positional_encoding_gaussian_matrix[0m
[34mproposal_generator.prompt_head.conv.0.{bias, weight}[0m
[34mproposal_generator.prompt_head.conv.1.{bias, running_mean, running_var, weight}[0m
[34mproposal_generator.prompt_head.mlp.1.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.3.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.5.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.objectness_logits.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.0.norm1.{bias, weight}[0m
[34mtransformer.layers.0.norm2.{bias, weight}[0m
[34mtransformer.layers.0.norm3.{bias, weight}[0m
[34mtransformer.layers.0.norm4.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.1.norm1.{bias, weight}[0m
[34mtransformer.layers.1.norm2.{bias, weight}[0m
[34mtransformer.layers.1.norm3.{bias, weight}[0m
[34mtransformer.layers.1.norm4.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.norm_final_attn.{bias, weight}[0m
[02/06 19:48:20] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.{bias, weight}[0m
  [35mfc1000.{bias, weight}[0m
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.shortcut.weight[0m
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv1.weight[0m
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv2.weight[0m
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv3.weight[0m
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv1.weight[0m
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv2.weight[0m
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv3.weight[0m
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv1.weight[0m
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv2.weight[0m
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv3.weight[0m
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.shortcut.weight[0m
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv1.weight[0m
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv2.weight[0m
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv3.weight[0m
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv1.weight[0m
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv2.weight[0m
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv3.weight[0m
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv1.weight[0m
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv2.weight[0m
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv3.weight[0m
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv1.weight[0m
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv2.weight[0m
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv3.weight[0m
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.shortcut.weight[0m
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv1.weight[0m
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv2.weight[0m
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv3.weight[0m
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv1.weight[0m
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv2.weight[0m
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv3.weight[0m
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv1.weight[0m
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv2.weight[0m
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv3.weight[0m
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv1.weight[0m
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv2.weight[0m
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv3.weight[0m
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv1.weight[0m
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv2.weight[0m
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv3.weight[0m
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv1.weight[0m
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv2.weight[0m
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv3.weight[0m
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.shortcut.weight[0m
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv1.weight[0m
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv2.weight[0m
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv3.weight[0m
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv1.weight[0m
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv2.weight[0m
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv3.weight[0m
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv1.weight[0m
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv2.weight[0m
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv3.weight[0m
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}[0m
[02/06 19:48:20] d2.engine.train_loop INFO: Starting training from iteration 0
[02/06 19:48:27] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 152, in train
    self.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 307, in run_step
    loss_dict = self.model(data)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/uwsam.py", line 137, in forward
    proposals, proposal_losses, all_visual_prompts, pred_class_logits, pred_proposal_deltas = self.proposal_generator(images, features, gt_instances)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/eupg.py", line 294, in forward
    gt_as_prop.objectness_logits = torch.ones(len(gt), device=p.device) * 10.0
  File "/home/hoangnv/detectron2/detectron2/structures/instances.py", line 66, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'device' in the given Instances!
[02/06 19:48:27] d2.engine.hooks INFO: Total training time: 0:00:06 (0:00:00 on hooks)
[02/06 19:48:27] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 16302M
[02/06 19:52:33] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 19:52:34] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 19:52:34] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 19:52:34] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 19:52:34] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 19:52:35] d2.utils.env INFO: Using a generated random seed 36692286
[02/06 19:53:16] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 19:53:16] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 19:53:26] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 10.70 seconds.
[02/06 19:53:27] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 19:53:30] d2.data.build INFO: Removed 0 images with no usable annotations. 8038 images left.
[02/06 19:53:30] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    fish    | 17238        |  reptiles  | 290          | arthropoda | 334          |
|   corals   | 9245         |  mollusk   | 1832         |   plants   | 1398         |
|   ruins    | 670          |  garbage   | 630          |   human    | 1511         |
|   robots   | 414          |            |              |            |              |
|   total    | 33562        |            |              |            |              |[0m
[02/06 19:53:30] d2.data.build INFO: Using training sampler TrainingSampler
[02/06 19:53:30] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/06 19:53:30] d2.data.common INFO: Serializing 8038 elements to byte tensors and concatenating them all ...
[02/06 19:53:32] d2.data.common INFO: Serialized dataset takes 138.37 MiB
[02/06 19:53:32] d2.data.build INFO: Making batched data loader with batch_size=4
[02/06 19:53:33] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 19:53:33] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/hoangnv/.torch/iopath_cache/detectron2/ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 19:53:33] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/06 19:53:33] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[02/06 19:53:33] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.vit.blocks.0.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm2.{bias, weight}[0m
[34mbackbone.vit.neck.0.weight[0m
[34mbackbone.vit.neck.1.{bias, weight}[0m
[34mbackbone.vit.neck.2.weight[0m
[34mbackbone.vit.neck.3.{bias, weight}[0m
[34mbackbone.vit.patch_embed.proj.{bias, weight}[0m
[34mbackbone.vit.pos_embed[0m
[34mmask_decoder.iou_prediction_head.layers.0.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.1.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.2.{bias, weight}[0m
[34mmask_decoder.iou_token.weight[0m
[34mmask_decoder.mask_tokens.weight[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.2.{bias, weight}[0m
[34mmask_decoder.output_upscaling.0.{bias, weight}[0m
[34mmask_decoder.output_upscaling.1.{bias, weight}[0m
[34mmask_decoder.output_upscaling.3.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.norm_final_attn.{bias, weight}[0m
[34mpe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.mask_downscaling.0.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.1.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.3.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.4.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.6.{bias, weight}[0m
[34mprompt_encoder.no_mask_embed.weight[0m
[34mprompt_encoder.not_a_point_embed.weight[0m
[34mprompt_encoder.pe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.point_embeddings.0.weight[0m
[34mprompt_encoder.point_embeddings.1.weight[0m
[34mprompt_encoder.point_embeddings.2.weight[0m
[34mprompt_encoder.point_embeddings.3.weight[0m
[34mproposal_generator.box_head.conv.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.4.{bias, weight}[0m
[34mproposal_generator.channel_attention.fc.0.weight[0m
[34mproposal_generator.channel_attention.fc.2.weight[0m
[34mproposal_generator.cls_head.conv.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p2_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p3_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p4_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.1.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.1.{bias, weight}[0m
[34mproposal_generator.pe_layer.positional_encoding_gaussian_matrix[0m
[34mproposal_generator.prompt_head.conv.0.{bias, weight}[0m
[34mproposal_generator.prompt_head.conv.1.{bias, running_mean, running_var, weight}[0m
[34mproposal_generator.prompt_head.mlp.1.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.3.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.5.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.objectness_logits.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.0.norm1.{bias, weight}[0m
[34mtransformer.layers.0.norm2.{bias, weight}[0m
[34mtransformer.layers.0.norm3.{bias, weight}[0m
[34mtransformer.layers.0.norm4.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.1.norm1.{bias, weight}[0m
[34mtransformer.layers.1.norm2.{bias, weight}[0m
[34mtransformer.layers.1.norm3.{bias, weight}[0m
[34mtransformer.layers.1.norm4.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.norm_final_attn.{bias, weight}[0m
[02/06 19:53:33] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.{bias, weight}[0m
  [35mfc1000.{bias, weight}[0m
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.shortcut.weight[0m
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv1.weight[0m
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv2.weight[0m
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv3.weight[0m
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv1.weight[0m
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv2.weight[0m
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv3.weight[0m
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv1.weight[0m
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv2.weight[0m
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv3.weight[0m
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.shortcut.weight[0m
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv1.weight[0m
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv2.weight[0m
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv3.weight[0m
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv1.weight[0m
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv2.weight[0m
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv3.weight[0m
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv1.weight[0m
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv2.weight[0m
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv3.weight[0m
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv1.weight[0m
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv2.weight[0m
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv3.weight[0m
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.shortcut.weight[0m
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv1.weight[0m
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv2.weight[0m
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv3.weight[0m
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv1.weight[0m
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv2.weight[0m
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv3.weight[0m
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv1.weight[0m
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv2.weight[0m
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv3.weight[0m
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv1.weight[0m
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv2.weight[0m
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv3.weight[0m
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv1.weight[0m
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv2.weight[0m
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv3.weight[0m
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv1.weight[0m
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv2.weight[0m
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv3.weight[0m
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.shortcut.weight[0m
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv1.weight[0m
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv2.weight[0m
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv3.weight[0m
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv1.weight[0m
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv2.weight[0m
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv3.weight[0m
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv1.weight[0m
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv2.weight[0m
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv3.weight[0m
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}[0m
[02/06 19:53:33] d2.engine.train_loop INFO: Starting training from iteration 0
[02/06 19:53:42] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 152, in train
    self.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 307, in run_step
    loss_dict = self.model(data)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/uwsam.py", line 177, in forward
    loss_cls_img = self.loss_cls(p_logits, gt_classes_target)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1362, in forward
    return F.cross_entropy(
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 3504, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[02/06 19:53:42] d2.engine.hooks INFO: Total training time: 0:00:08 (0:00:00 on hooks)
[02/06 19:53:42] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 16302M
[02/06 19:59:07] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 19:59:08] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 19:59:08] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 19:59:08] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 19:59:08] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 19:59:08] d2.utils.env INFO: Using a generated random seed 9937393
[02/06 19:59:33] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 19:59:33] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 19:59:38] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 5.48 seconds.
[02/06 19:59:38] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 19:59:40] d2.data.build INFO: Removed 0 images with no usable annotations. 8038 images left.
[02/06 19:59:41] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    fish    | 17238        |  reptiles  | 290          | arthropoda | 334          |
|   corals   | 9245         |  mollusk   | 1832         |   plants   | 1398         |
|   ruins    | 670          |  garbage   | 630          |   human    | 1511         |
|   robots   | 414          |            |              |            |              |
|   total    | 33562        |            |              |            |              |[0m
[02/06 19:59:41] d2.data.build INFO: Using training sampler TrainingSampler
[02/06 19:59:41] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/06 19:59:41] d2.data.common INFO: Serializing 8038 elements to byte tensors and concatenating them all ...
[02/06 19:59:42] d2.data.common INFO: Serialized dataset takes 138.37 MiB
[02/06 19:59:42] d2.data.build INFO: Making batched data loader with batch_size=4
[02/06 19:59:42] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 19:59:42] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/hoangnv/.torch/iopath_cache/detectron2/ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 19:59:43] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/06 19:59:43] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[02/06 19:59:43] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.vit.blocks.0.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm2.{bias, weight}[0m
[34mbackbone.vit.neck.0.weight[0m
[34mbackbone.vit.neck.1.{bias, weight}[0m
[34mbackbone.vit.neck.2.weight[0m
[34mbackbone.vit.neck.3.{bias, weight}[0m
[34mbackbone.vit.patch_embed.proj.{bias, weight}[0m
[34mbackbone.vit.pos_embed[0m
[34mmask_decoder.iou_prediction_head.layers.0.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.1.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.2.{bias, weight}[0m
[34mmask_decoder.iou_token.weight[0m
[34mmask_decoder.mask_tokens.weight[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.2.{bias, weight}[0m
[34mmask_decoder.output_upscaling.0.{bias, weight}[0m
[34mmask_decoder.output_upscaling.1.{bias, weight}[0m
[34mmask_decoder.output_upscaling.3.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.norm_final_attn.{bias, weight}[0m
[34mpe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.mask_downscaling.0.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.1.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.3.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.4.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.6.{bias, weight}[0m
[34mprompt_encoder.no_mask_embed.weight[0m
[34mprompt_encoder.not_a_point_embed.weight[0m
[34mprompt_encoder.pe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.point_embeddings.0.weight[0m
[34mprompt_encoder.point_embeddings.1.weight[0m
[34mprompt_encoder.point_embeddings.2.weight[0m
[34mprompt_encoder.point_embeddings.3.weight[0m
[34mproposal_generator.box_head.conv.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.4.{bias, weight}[0m
[34mproposal_generator.channel_attention.fc.0.weight[0m
[34mproposal_generator.channel_attention.fc.2.weight[0m
[34mproposal_generator.cls_head.conv.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p2_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p3_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p4_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.1.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.1.{bias, weight}[0m
[34mproposal_generator.pe_layer.positional_encoding_gaussian_matrix[0m
[34mproposal_generator.prompt_head.conv.0.{bias, weight}[0m
[34mproposal_generator.prompt_head.conv.1.{bias, running_mean, running_var, weight}[0m
[34mproposal_generator.prompt_head.mlp.1.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.3.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.5.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.objectness_logits.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.0.norm1.{bias, weight}[0m
[34mtransformer.layers.0.norm2.{bias, weight}[0m
[34mtransformer.layers.0.norm3.{bias, weight}[0m
[34mtransformer.layers.0.norm4.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.1.norm1.{bias, weight}[0m
[34mtransformer.layers.1.norm2.{bias, weight}[0m
[34mtransformer.layers.1.norm3.{bias, weight}[0m
[34mtransformer.layers.1.norm4.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.norm_final_attn.{bias, weight}[0m
[02/06 19:59:43] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.{bias, weight}[0m
  [35mfc1000.{bias, weight}[0m
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.shortcut.weight[0m
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv1.weight[0m
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv2.weight[0m
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv3.weight[0m
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv1.weight[0m
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv2.weight[0m
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv3.weight[0m
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv1.weight[0m
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv2.weight[0m
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv3.weight[0m
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.shortcut.weight[0m
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv1.weight[0m
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv2.weight[0m
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv3.weight[0m
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv1.weight[0m
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv2.weight[0m
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv3.weight[0m
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv1.weight[0m
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv2.weight[0m
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv3.weight[0m
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv1.weight[0m
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv2.weight[0m
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv3.weight[0m
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.shortcut.weight[0m
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv1.weight[0m
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv2.weight[0m
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv3.weight[0m
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv1.weight[0m
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv2.weight[0m
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv3.weight[0m
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv1.weight[0m
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv2.weight[0m
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv3.weight[0m
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv1.weight[0m
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv2.weight[0m
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv3.weight[0m
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv1.weight[0m
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv2.weight[0m
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv3.weight[0m
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv1.weight[0m
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv2.weight[0m
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv3.weight[0m
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.shortcut.weight[0m
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv1.weight[0m
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv2.weight[0m
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv3.weight[0m
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv1.weight[0m
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv2.weight[0m
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv3.weight[0m
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv1.weight[0m
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv2.weight[0m
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv3.weight[0m
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}[0m
[02/06 19:59:43] d2.engine.train_loop INFO: Starting training from iteration 0
[02/06 19:59:48] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 152, in train
    self.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 307, in run_step
    loss_dict = self.model(data)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/uwsam.py", line 176, in forward
    target_labels = torch.clamp(target_labels, min=0, max=self.num_classes - 1)
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[02/06 19:59:48] d2.engine.hooks INFO: Total training time: 0:00:05 (0:00:00 on hooks)
[02/06 19:59:48] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 16302M
[02/06 20:03:10] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 20:03:10] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 20:03:10] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 20:03:10] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 20:03:10] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 20:03:11] d2.utils.env INFO: Using a generated random seed 12636378
[02/06 20:03:49] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 20:03:49] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 20:03:57] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 7.56 seconds.
[02/06 20:03:57] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 20:04:00] d2.data.build INFO: Removed 0 images with no usable annotations. 8038 images left.
[02/06 20:04:00] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    fish    | 17238        |  reptiles  | 290          | arthropoda | 334          |
|   corals   | 9245         |  mollusk   | 1832         |   plants   | 1398         |
|   ruins    | 670          |  garbage   | 630          |   human    | 1511         |
|   robots   | 414          |            |              |            |              |
|   total    | 33562        |            |              |            |              |[0m
[02/06 20:04:00] d2.data.build INFO: Using training sampler TrainingSampler
[02/06 20:04:00] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/06 20:04:00] d2.data.common INFO: Serializing 8038 elements to byte tensors and concatenating them all ...
[02/06 20:04:02] d2.data.common INFO: Serialized dataset takes 138.37 MiB
[02/06 20:04:02] d2.data.build INFO: Making batched data loader with batch_size=4
[02/06 20:04:03] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:04:03] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/hoangnv/.torch/iopath_cache/detectron2/ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:04:03] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/06 20:04:03] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[02/06 20:04:04] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.vit.blocks.0.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm2.{bias, weight}[0m
[34mbackbone.vit.neck.0.weight[0m
[34mbackbone.vit.neck.1.{bias, weight}[0m
[34mbackbone.vit.neck.2.weight[0m
[34mbackbone.vit.neck.3.{bias, weight}[0m
[34mbackbone.vit.patch_embed.proj.{bias, weight}[0m
[34mbackbone.vit.pos_embed[0m
[34mmask_decoder.iou_prediction_head.layers.0.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.1.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.2.{bias, weight}[0m
[34mmask_decoder.iou_token.weight[0m
[34mmask_decoder.mask_tokens.weight[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.2.{bias, weight}[0m
[34mmask_decoder.output_upscaling.0.{bias, weight}[0m
[34mmask_decoder.output_upscaling.1.{bias, weight}[0m
[34mmask_decoder.output_upscaling.3.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.norm_final_attn.{bias, weight}[0m
[34mpe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.mask_downscaling.0.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.1.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.3.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.4.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.6.{bias, weight}[0m
[34mprompt_encoder.no_mask_embed.weight[0m
[34mprompt_encoder.not_a_point_embed.weight[0m
[34mprompt_encoder.pe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.point_embeddings.0.weight[0m
[34mprompt_encoder.point_embeddings.1.weight[0m
[34mprompt_encoder.point_embeddings.2.weight[0m
[34mprompt_encoder.point_embeddings.3.weight[0m
[34mproposal_generator.box_head.conv.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.4.{bias, weight}[0m
[34mproposal_generator.channel_attention.fc.0.weight[0m
[34mproposal_generator.channel_attention.fc.2.weight[0m
[34mproposal_generator.cls_head.conv.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p2_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p3_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p4_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.1.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.1.{bias, weight}[0m
[34mproposal_generator.pe_layer.positional_encoding_gaussian_matrix[0m
[34mproposal_generator.prompt_head.conv.0.{bias, weight}[0m
[34mproposal_generator.prompt_head.conv.1.{bias, running_mean, running_var, weight}[0m
[34mproposal_generator.prompt_head.mlp.1.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.3.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.5.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.objectness_logits.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.0.norm1.{bias, weight}[0m
[34mtransformer.layers.0.norm2.{bias, weight}[0m
[34mtransformer.layers.0.norm3.{bias, weight}[0m
[34mtransformer.layers.0.norm4.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.1.norm1.{bias, weight}[0m
[34mtransformer.layers.1.norm2.{bias, weight}[0m
[34mtransformer.layers.1.norm3.{bias, weight}[0m
[34mtransformer.layers.1.norm4.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.norm_final_attn.{bias, weight}[0m
[02/06 20:04:04] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.{bias, weight}[0m
  [35mfc1000.{bias, weight}[0m
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.shortcut.weight[0m
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv1.weight[0m
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv2.weight[0m
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv3.weight[0m
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv1.weight[0m
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv2.weight[0m
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv3.weight[0m
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv1.weight[0m
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv2.weight[0m
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv3.weight[0m
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.shortcut.weight[0m
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv1.weight[0m
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv2.weight[0m
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv3.weight[0m
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv1.weight[0m
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv2.weight[0m
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv3.weight[0m
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv1.weight[0m
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv2.weight[0m
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv3.weight[0m
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv1.weight[0m
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv2.weight[0m
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv3.weight[0m
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.shortcut.weight[0m
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv1.weight[0m
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv2.weight[0m
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv3.weight[0m
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv1.weight[0m
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv2.weight[0m
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv3.weight[0m
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv1.weight[0m
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv2.weight[0m
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv3.weight[0m
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv1.weight[0m
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv2.weight[0m
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv3.weight[0m
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv1.weight[0m
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv2.weight[0m
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv3.weight[0m
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv1.weight[0m
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv2.weight[0m
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv3.weight[0m
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.shortcut.weight[0m
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv1.weight[0m
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv2.weight[0m
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv3.weight[0m
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv1.weight[0m
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv2.weight[0m
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv3.weight[0m
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv1.weight[0m
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv2.weight[0m
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv3.weight[0m
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}[0m
[02/06 20:04:04] d2.engine.train_loop INFO: Starting training from iteration 0
[02/06 20:04:08] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 152, in train
    self.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 307, in run_step
    loss_dict = self.model(data)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/uwsam.py", line 182, in forward
    loss_cls_img = self.loss_cls(p_logits, gt_classes_target)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1362, in forward
    return F.cross_entropy(
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 3504, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[02/06 20:04:08] d2.engine.hooks INFO: Total training time: 0:00:04 (0:00:00 on hooks)
[02/06 20:04:08] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 16302M
[02/06 20:07:37] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 20:07:38] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 20:07:38] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 20:07:38] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 20:07:38] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 20:07:38] d2.utils.env INFO: Using a generated random seed 40493464
[02/06 20:08:11] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 20:08:11] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 20:08:18] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 6.80 seconds.
[02/06 20:08:18] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 20:08:21] d2.data.build INFO: Removed 0 images with no usable annotations. 8038 images left.
[02/06 20:08:21] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    fish    | 17238        |  reptiles  | 290          | arthropoda | 334          |
|   corals   | 9245         |  mollusk   | 1832         |   plants   | 1398         |
|   ruins    | 670          |  garbage   | 630          |   human    | 1511         |
|   robots   | 414          |            |              |            |              |
|   total    | 33562        |            |              |            |              |[0m
[02/06 20:08:21] d2.data.build INFO: Using training sampler TrainingSampler
[02/06 20:08:21] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/06 20:08:21] d2.data.common INFO: Serializing 8038 elements to byte tensors and concatenating them all ...
[02/06 20:08:23] d2.data.common INFO: Serialized dataset takes 138.37 MiB
[02/06 20:08:23] d2.data.build INFO: Making batched data loader with batch_size=4
[02/06 20:08:24] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:08:24] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/hoangnv/.torch/iopath_cache/detectron2/ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:08:24] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/06 20:08:24] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[02/06 20:08:24] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.vit.blocks.0.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm2.{bias, weight}[0m
[34mbackbone.vit.neck.0.weight[0m
[34mbackbone.vit.neck.1.{bias, weight}[0m
[34mbackbone.vit.neck.2.weight[0m
[34mbackbone.vit.neck.3.{bias, weight}[0m
[34mbackbone.vit.patch_embed.proj.{bias, weight}[0m
[34mbackbone.vit.pos_embed[0m
[34mmask_decoder.iou_prediction_head.layers.0.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.1.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.2.{bias, weight}[0m
[34mmask_decoder.iou_token.weight[0m
[34mmask_decoder.mask_tokens.weight[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.2.{bias, weight}[0m
[34mmask_decoder.output_upscaling.0.{bias, weight}[0m
[34mmask_decoder.output_upscaling.1.{bias, weight}[0m
[34mmask_decoder.output_upscaling.3.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.norm_final_attn.{bias, weight}[0m
[34mpe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.mask_downscaling.0.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.1.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.3.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.4.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.6.{bias, weight}[0m
[34mprompt_encoder.no_mask_embed.weight[0m
[34mprompt_encoder.not_a_point_embed.weight[0m
[34mprompt_encoder.pe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.point_embeddings.0.weight[0m
[34mprompt_encoder.point_embeddings.1.weight[0m
[34mprompt_encoder.point_embeddings.2.weight[0m
[34mprompt_encoder.point_embeddings.3.weight[0m
[34mproposal_generator.box_head.conv.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.4.{bias, weight}[0m
[34mproposal_generator.channel_attention.fc.0.weight[0m
[34mproposal_generator.channel_attention.fc.2.weight[0m
[34mproposal_generator.cls_head.conv.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p2_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p3_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p4_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.1.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.1.{bias, weight}[0m
[34mproposal_generator.pe_layer.positional_encoding_gaussian_matrix[0m
[34mproposal_generator.prompt_head.conv.0.{bias, weight}[0m
[34mproposal_generator.prompt_head.conv.1.{bias, running_mean, running_var, weight}[0m
[34mproposal_generator.prompt_head.mlp.1.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.3.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.5.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.objectness_logits.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.0.norm1.{bias, weight}[0m
[34mtransformer.layers.0.norm2.{bias, weight}[0m
[34mtransformer.layers.0.norm3.{bias, weight}[0m
[34mtransformer.layers.0.norm4.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.1.norm1.{bias, weight}[0m
[34mtransformer.layers.1.norm2.{bias, weight}[0m
[34mtransformer.layers.1.norm3.{bias, weight}[0m
[34mtransformer.layers.1.norm4.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.norm_final_attn.{bias, weight}[0m
[02/06 20:08:24] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.{bias, weight}[0m
  [35mfc1000.{bias, weight}[0m
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.shortcut.weight[0m
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv1.weight[0m
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv2.weight[0m
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv3.weight[0m
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv1.weight[0m
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv2.weight[0m
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv3.weight[0m
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv1.weight[0m
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv2.weight[0m
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv3.weight[0m
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.shortcut.weight[0m
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv1.weight[0m
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv2.weight[0m
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv3.weight[0m
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv1.weight[0m
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv2.weight[0m
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv3.weight[0m
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv1.weight[0m
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv2.weight[0m
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv3.weight[0m
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv1.weight[0m
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv2.weight[0m
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv3.weight[0m
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.shortcut.weight[0m
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv1.weight[0m
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv2.weight[0m
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv3.weight[0m
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv1.weight[0m
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv2.weight[0m
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv3.weight[0m
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv1.weight[0m
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv2.weight[0m
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv3.weight[0m
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv1.weight[0m
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv2.weight[0m
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv3.weight[0m
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv1.weight[0m
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv2.weight[0m
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv3.weight[0m
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv1.weight[0m
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv2.weight[0m
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv3.weight[0m
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.shortcut.weight[0m
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv1.weight[0m
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv2.weight[0m
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv3.weight[0m
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv1.weight[0m
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv2.weight[0m
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv3.weight[0m
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv1.weight[0m
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv2.weight[0m
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv3.weight[0m
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}[0m
[02/06 20:08:24] d2.engine.train_loop INFO: Starting training from iteration 0
[02/06 20:08:33] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 152, in train
    self.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 307, in run_step
    loss_dict = self.model(data)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/uwsam.py", line 186, in forward
    loss_cls_img = self.loss_cls(p_logits, gt_classes_target)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1362, in forward
    return F.cross_entropy(
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 3504, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[02/06 20:08:33] d2.engine.hooks INFO: Total training time: 0:00:08 (0:00:00 on hooks)
[02/06 20:08:33] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 16302M
[02/06 20:10:25] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 20:10:26] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 20:10:26] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 20:10:26] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 20:10:26] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 20:10:26] d2.utils.env INFO: Using a generated random seed 27908909
[02/06 20:10:58] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 20:10:58] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 20:11:06] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 7.65 seconds.
[02/06 20:11:06] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 20:11:08] d2.data.build INFO: Removed 0 images with no usable annotations. 8038 images left.
[02/06 20:11:08] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    fish    | 17238        |  reptiles  | 290          | arthropoda | 334          |
|   corals   | 9245         |  mollusk   | 1832         |   plants   | 1398         |
|   ruins    | 670          |  garbage   | 630          |   human    | 1511         |
|   robots   | 414          |            |              |            |              |
|   total    | 33562        |            |              |            |              |[0m
[02/06 20:11:08] d2.data.build INFO: Using training sampler TrainingSampler
[02/06 20:11:08] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/06 20:11:08] d2.data.common INFO: Serializing 8038 elements to byte tensors and concatenating them all ...
[02/06 20:11:10] d2.data.common INFO: Serialized dataset takes 138.37 MiB
[02/06 20:11:10] d2.data.build INFO: Making batched data loader with batch_size=4
[02/06 20:11:11] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:11:11] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/hoangnv/.torch/iopath_cache/detectron2/ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:11:11] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/06 20:11:11] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[02/06 20:11:11] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.vit.blocks.0.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm2.{bias, weight}[0m
[34mbackbone.vit.neck.0.weight[0m
[34mbackbone.vit.neck.1.{bias, weight}[0m
[34mbackbone.vit.neck.2.weight[0m
[34mbackbone.vit.neck.3.{bias, weight}[0m
[34mbackbone.vit.patch_embed.proj.{bias, weight}[0m
[34mbackbone.vit.pos_embed[0m
[34mmask_decoder.iou_prediction_head.layers.0.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.1.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.2.{bias, weight}[0m
[34mmask_decoder.iou_token.weight[0m
[34mmask_decoder.mask_tokens.weight[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.2.{bias, weight}[0m
[34mmask_decoder.output_upscaling.0.{bias, weight}[0m
[34mmask_decoder.output_upscaling.1.{bias, weight}[0m
[34mmask_decoder.output_upscaling.3.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.norm_final_attn.{bias, weight}[0m
[34mpe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.mask_downscaling.0.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.1.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.3.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.4.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.6.{bias, weight}[0m
[34mprompt_encoder.no_mask_embed.weight[0m
[34mprompt_encoder.not_a_point_embed.weight[0m
[34mprompt_encoder.pe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.point_embeddings.0.weight[0m
[34mprompt_encoder.point_embeddings.1.weight[0m
[34mprompt_encoder.point_embeddings.2.weight[0m
[34mprompt_encoder.point_embeddings.3.weight[0m
[34mproposal_generator.box_head.conv.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.4.{bias, weight}[0m
[34mproposal_generator.channel_attention.fc.0.weight[0m
[34mproposal_generator.channel_attention.fc.2.weight[0m
[34mproposal_generator.cls_head.conv.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p2_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p3_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p4_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.1.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.1.{bias, weight}[0m
[34mproposal_generator.pe_layer.positional_encoding_gaussian_matrix[0m
[34mproposal_generator.prompt_head.conv.0.{bias, weight}[0m
[34mproposal_generator.prompt_head.conv.1.{bias, running_mean, running_var, weight}[0m
[34mproposal_generator.prompt_head.mlp.1.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.3.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.5.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.objectness_logits.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.0.norm1.{bias, weight}[0m
[34mtransformer.layers.0.norm2.{bias, weight}[0m
[34mtransformer.layers.0.norm3.{bias, weight}[0m
[34mtransformer.layers.0.norm4.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.1.norm1.{bias, weight}[0m
[34mtransformer.layers.1.norm2.{bias, weight}[0m
[34mtransformer.layers.1.norm3.{bias, weight}[0m
[34mtransformer.layers.1.norm4.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.norm_final_attn.{bias, weight}[0m
[02/06 20:11:11] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.{bias, weight}[0m
  [35mfc1000.{bias, weight}[0m
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.shortcut.weight[0m
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv1.weight[0m
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv2.weight[0m
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv3.weight[0m
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv1.weight[0m
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv2.weight[0m
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv3.weight[0m
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv1.weight[0m
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv2.weight[0m
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv3.weight[0m
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.shortcut.weight[0m
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv1.weight[0m
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv2.weight[0m
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv3.weight[0m
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv1.weight[0m
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv2.weight[0m
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv3.weight[0m
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv1.weight[0m
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv2.weight[0m
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv3.weight[0m
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv1.weight[0m
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv2.weight[0m
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv3.weight[0m
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.shortcut.weight[0m
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv1.weight[0m
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv2.weight[0m
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv3.weight[0m
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv1.weight[0m
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv2.weight[0m
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv3.weight[0m
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv1.weight[0m
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv2.weight[0m
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv3.weight[0m
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv1.weight[0m
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv2.weight[0m
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv3.weight[0m
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv1.weight[0m
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv2.weight[0m
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv3.weight[0m
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv1.weight[0m
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv2.weight[0m
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv3.weight[0m
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.shortcut.weight[0m
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv1.weight[0m
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv2.weight[0m
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv3.weight[0m
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv1.weight[0m
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv2.weight[0m
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv3.weight[0m
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv1.weight[0m
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv2.weight[0m
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv3.weight[0m
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}[0m
[02/06 20:11:11] d2.engine.train_loop INFO: Starting training from iteration 0
[02/06 20:11:18] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 152, in train
    self.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 307, in run_step
    loss_dict = self.model(data)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/uwsam.py", line 208, in forward
    loss_cls_img = self.loss_cls(p_logits, gt_classes_target)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1362, in forward
    return F.cross_entropy(
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 3504, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[02/06 20:11:18] d2.engine.hooks INFO: Total training time: 0:00:06 (0:00:00 on hooks)
[02/06 20:11:18] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 16302M
[02/06 20:14:57] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 20:14:57] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 20:14:57] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 20:14:58] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 20:14:58] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 20:14:58] d2.utils.env INFO: Using a generated random seed 59831144
[02/06 20:15:38] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 20:15:38] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 20:15:48] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 9.99 seconds.
[02/06 20:15:48] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 20:15:51] d2.data.build INFO: Removed 0 images with no usable annotations. 8038 images left.
[02/06 20:15:52] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    fish    | 17238        |  reptiles  | 290          | arthropoda | 334          |
|   corals   | 9245         |  mollusk   | 1832         |   plants   | 1398         |
|   ruins    | 670          |  garbage   | 630          |   human    | 1511         |
|   robots   | 414          |            |              |            |              |
|   total    | 33562        |            |              |            |              |[0m
[02/06 20:15:52] d2.data.build INFO: Using training sampler TrainingSampler
[02/06 20:15:52] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/06 20:15:52] d2.data.common INFO: Serializing 8038 elements to byte tensors and concatenating them all ...
[02/06 20:15:54] d2.data.common INFO: Serialized dataset takes 138.37 MiB
[02/06 20:15:54] d2.data.build INFO: Making batched data loader with batch_size=4
[02/06 20:15:54] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:15:54] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/hoangnv/.torch/iopath_cache/detectron2/ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:15:55] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/06 20:15:55] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[02/06 20:15:55] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.vit.blocks.0.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm2.{bias, weight}[0m
[34mbackbone.vit.neck.0.weight[0m
[34mbackbone.vit.neck.1.{bias, weight}[0m
[34mbackbone.vit.neck.2.weight[0m
[34mbackbone.vit.neck.3.{bias, weight}[0m
[34mbackbone.vit.patch_embed.proj.{bias, weight}[0m
[34mbackbone.vit.pos_embed[0m
[34mmask_decoder.iou_prediction_head.layers.0.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.1.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.2.{bias, weight}[0m
[34mmask_decoder.iou_token.weight[0m
[34mmask_decoder.mask_tokens.weight[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.2.{bias, weight}[0m
[34mmask_decoder.output_upscaling.0.{bias, weight}[0m
[34mmask_decoder.output_upscaling.1.{bias, weight}[0m
[34mmask_decoder.output_upscaling.3.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.norm_final_attn.{bias, weight}[0m
[34mpe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.mask_downscaling.0.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.1.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.3.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.4.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.6.{bias, weight}[0m
[34mprompt_encoder.no_mask_embed.weight[0m
[34mprompt_encoder.not_a_point_embed.weight[0m
[34mprompt_encoder.pe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.point_embeddings.0.weight[0m
[34mprompt_encoder.point_embeddings.1.weight[0m
[34mprompt_encoder.point_embeddings.2.weight[0m
[34mprompt_encoder.point_embeddings.3.weight[0m
[34mproposal_generator.box_head.conv.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.4.{bias, weight}[0m
[34mproposal_generator.channel_attention.fc.0.weight[0m
[34mproposal_generator.channel_attention.fc.2.weight[0m
[34mproposal_generator.cls_head.conv.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p2_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p3_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p4_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.1.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.1.{bias, weight}[0m
[34mproposal_generator.pe_layer.positional_encoding_gaussian_matrix[0m
[34mproposal_generator.prompt_head.conv.0.{bias, weight}[0m
[34mproposal_generator.prompt_head.conv.1.{bias, running_mean, running_var, weight}[0m
[34mproposal_generator.prompt_head.mlp.1.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.3.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.5.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.objectness_logits.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.0.norm1.{bias, weight}[0m
[34mtransformer.layers.0.norm2.{bias, weight}[0m
[34mtransformer.layers.0.norm3.{bias, weight}[0m
[34mtransformer.layers.0.norm4.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.1.norm1.{bias, weight}[0m
[34mtransformer.layers.1.norm2.{bias, weight}[0m
[34mtransformer.layers.1.norm3.{bias, weight}[0m
[34mtransformer.layers.1.norm4.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.norm_final_attn.{bias, weight}[0m
[02/06 20:15:55] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.{bias, weight}[0m
  [35mfc1000.{bias, weight}[0m
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.shortcut.weight[0m
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv1.weight[0m
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv2.weight[0m
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv3.weight[0m
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv1.weight[0m
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv2.weight[0m
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv3.weight[0m
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv1.weight[0m
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv2.weight[0m
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv3.weight[0m
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.shortcut.weight[0m
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv1.weight[0m
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv2.weight[0m
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv3.weight[0m
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv1.weight[0m
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv2.weight[0m
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv3.weight[0m
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv1.weight[0m
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv2.weight[0m
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv3.weight[0m
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv1.weight[0m
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv2.weight[0m
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv3.weight[0m
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.shortcut.weight[0m
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv1.weight[0m
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv2.weight[0m
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv3.weight[0m
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv1.weight[0m
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv2.weight[0m
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv3.weight[0m
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv1.weight[0m
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv2.weight[0m
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv3.weight[0m
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv1.weight[0m
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv2.weight[0m
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv3.weight[0m
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv1.weight[0m
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv2.weight[0m
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv3.weight[0m
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv1.weight[0m
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv2.weight[0m
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv3.weight[0m
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.shortcut.weight[0m
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv1.weight[0m
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv2.weight[0m
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv3.weight[0m
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv1.weight[0m
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv2.weight[0m
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv3.weight[0m
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv1.weight[0m
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv2.weight[0m
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv3.weight[0m
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}[0m
[02/06 20:15:55] d2.engine.train_loop INFO: Starting training from iteration 0
[02/06 20:16:03] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 152, in train
    self.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 307, in run_step
    loss_dict = self.model(data)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/uwsam.py", line 199, in forward
    loss_cls_img = self.loss_cls(p_logits, gt_classes_target)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1362, in forward
    return F.cross_entropy(
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 3504, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[02/06 20:16:03] d2.engine.hooks INFO: Total training time: 0:00:07 (0:00:00 on hooks)
[02/06 20:16:03] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 16302M
[02/06 20:18:22] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 20:18:23] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 20:18:23] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 20:18:23] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 20:18:23] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 20:18:24] d2.utils.env INFO: Using a generated random seed 25670852
[02/06 20:19:00] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 20:19:00] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 20:19:10] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 10.35 seconds.
[02/06 20:19:10] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 20:19:13] d2.data.build INFO: Removed 0 images with no usable annotations. 8038 images left.
[02/06 20:19:13] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    fish    | 17238        |  reptiles  | 290          | arthropoda | 334          |
|   corals   | 9245         |  mollusk   | 1832         |   plants   | 1398         |
|   ruins    | 670          |  garbage   | 630          |   human    | 1511         |
|   robots   | 414          |            |              |            |              |
|   total    | 33562        |            |              |            |              |[0m
[02/06 20:19:13] d2.data.build INFO: Using training sampler TrainingSampler
[02/06 20:19:13] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/06 20:19:13] d2.data.common INFO: Serializing 8038 elements to byte tensors and concatenating them all ...
[02/06 20:19:14] d2.data.common INFO: Serialized dataset takes 138.37 MiB
[02/06 20:19:14] d2.data.build INFO: Making batched data loader with batch_size=4
[02/06 20:19:15] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:19:15] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/hoangnv/.torch/iopath_cache/detectron2/ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:19:15] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/06 20:19:15] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[02/06 20:19:15] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.vit.blocks.0.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm2.{bias, weight}[0m
[34mbackbone.vit.neck.0.weight[0m
[34mbackbone.vit.neck.1.{bias, weight}[0m
[34mbackbone.vit.neck.2.weight[0m
[34mbackbone.vit.neck.3.{bias, weight}[0m
[34mbackbone.vit.patch_embed.proj.{bias, weight}[0m
[34mbackbone.vit.pos_embed[0m
[34mmask_decoder.iou_prediction_head.layers.0.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.1.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.2.{bias, weight}[0m
[34mmask_decoder.iou_token.weight[0m
[34mmask_decoder.mask_tokens.weight[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.2.{bias, weight}[0m
[34mmask_decoder.output_upscaling.0.{bias, weight}[0m
[34mmask_decoder.output_upscaling.1.{bias, weight}[0m
[34mmask_decoder.output_upscaling.3.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.norm_final_attn.{bias, weight}[0m
[34mpe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.mask_downscaling.0.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.1.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.3.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.4.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.6.{bias, weight}[0m
[34mprompt_encoder.no_mask_embed.weight[0m
[34mprompt_encoder.not_a_point_embed.weight[0m
[34mprompt_encoder.pe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.point_embeddings.0.weight[0m
[34mprompt_encoder.point_embeddings.1.weight[0m
[34mprompt_encoder.point_embeddings.2.weight[0m
[34mprompt_encoder.point_embeddings.3.weight[0m
[34mproposal_generator.box_head.conv.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.4.{bias, weight}[0m
[34mproposal_generator.channel_attention.fc.0.weight[0m
[34mproposal_generator.channel_attention.fc.2.weight[0m
[34mproposal_generator.cls_head.conv.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p2_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p3_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p4_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.1.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.1.{bias, weight}[0m
[34mproposal_generator.pe_layer.positional_encoding_gaussian_matrix[0m
[34mproposal_generator.prompt_head.conv.0.{bias, weight}[0m
[34mproposal_generator.prompt_head.conv.1.{bias, running_mean, running_var, weight}[0m
[34mproposal_generator.prompt_head.mlp.1.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.3.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.5.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.objectness_logits.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.0.norm1.{bias, weight}[0m
[34mtransformer.layers.0.norm2.{bias, weight}[0m
[34mtransformer.layers.0.norm3.{bias, weight}[0m
[34mtransformer.layers.0.norm4.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.1.norm1.{bias, weight}[0m
[34mtransformer.layers.1.norm2.{bias, weight}[0m
[34mtransformer.layers.1.norm3.{bias, weight}[0m
[34mtransformer.layers.1.norm4.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.norm_final_attn.{bias, weight}[0m
[02/06 20:19:15] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.{bias, weight}[0m
  [35mfc1000.{bias, weight}[0m
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.shortcut.weight[0m
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv1.weight[0m
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv2.weight[0m
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv3.weight[0m
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv1.weight[0m
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv2.weight[0m
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv3.weight[0m
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv1.weight[0m
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv2.weight[0m
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv3.weight[0m
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.shortcut.weight[0m
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv1.weight[0m
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv2.weight[0m
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv3.weight[0m
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv1.weight[0m
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv2.weight[0m
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv3.weight[0m
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv1.weight[0m
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv2.weight[0m
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv3.weight[0m
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv1.weight[0m
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv2.weight[0m
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv3.weight[0m
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.shortcut.weight[0m
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv1.weight[0m
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv2.weight[0m
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv3.weight[0m
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv1.weight[0m
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv2.weight[0m
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv3.weight[0m
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv1.weight[0m
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv2.weight[0m
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv3.weight[0m
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv1.weight[0m
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv2.weight[0m
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv3.weight[0m
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv1.weight[0m
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv2.weight[0m
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv3.weight[0m
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv1.weight[0m
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv2.weight[0m
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv3.weight[0m
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.shortcut.weight[0m
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv1.weight[0m
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv2.weight[0m
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv3.weight[0m
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv1.weight[0m
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv2.weight[0m
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv3.weight[0m
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv1.weight[0m
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv2.weight[0m
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv3.weight[0m
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}[0m
[02/06 20:19:15] d2.engine.train_loop INFO: Starting training from iteration 0
[02/06 20:19:23] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 152, in train
    self.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 307, in run_step
    loss_dict = self.model(data)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/uwsam.py", line 197, in forward
    loss_cls_img = self.loss_cls(p_logits, gt_classes_target)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1362, in forward
    return F.cross_entropy(
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 3504, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[02/06 20:19:23] d2.engine.hooks INFO: Total training time: 0:00:07 (0:00:00 on hooks)
[02/06 20:19:23] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 16302M
[02/06 20:21:31] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 20:21:31] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 20:21:31] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 20:21:31] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 20:21:31] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 20:21:32] d2.utils.env INFO: Using a generated random seed 33691890
[02/06 20:22:09] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 20:22:09] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 20:22:53] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 20:22:54] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 20:22:54] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 20:22:54] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 20:22:54] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 20:22:54] d2.utils.env INFO: Using a generated random seed 56063731
[02/06 20:23:42] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 20:23:42] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 20:23:53] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 10.40 seconds.
[02/06 20:23:53] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 20:23:56] d2.data.build INFO: Removed 0 images with no usable annotations. 8038 images left.
[02/06 20:23:57] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    fish    | 17238        |  reptiles  | 290          | arthropoda | 334          |
|   corals   | 9245         |  mollusk   | 1832         |   plants   | 1398         |
|   ruins    | 670          |  garbage   | 630          |   human    | 1511         |
|   robots   | 414          |            |              |            |              |
|   total    | 33562        |            |              |            |              |[0m
[02/06 20:23:57] d2.data.build INFO: Using training sampler TrainingSampler
[02/06 20:23:57] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/06 20:23:57] d2.data.common INFO: Serializing 8038 elements to byte tensors and concatenating them all ...
[02/06 20:23:59] d2.data.common INFO: Serialized dataset takes 138.37 MiB
[02/06 20:23:59] d2.data.build INFO: Making batched data loader with batch_size=4
[02/06 20:24:00] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:24:00] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/hoangnv/.torch/iopath_cache/detectron2/ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:24:01] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/06 20:24:01] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[02/06 20:24:01] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.vit.blocks.0.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm2.{bias, weight}[0m
[34mbackbone.vit.neck.0.weight[0m
[34mbackbone.vit.neck.1.{bias, weight}[0m
[34mbackbone.vit.neck.2.weight[0m
[34mbackbone.vit.neck.3.{bias, weight}[0m
[34mbackbone.vit.patch_embed.proj.{bias, weight}[0m
[34mbackbone.vit.pos_embed[0m
[34mmask_decoder.iou_prediction_head.layers.0.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.1.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.2.{bias, weight}[0m
[34mmask_decoder.iou_token.weight[0m
[34mmask_decoder.mask_tokens.weight[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.2.{bias, weight}[0m
[34mmask_decoder.output_upscaling.0.{bias, weight}[0m
[34mmask_decoder.output_upscaling.1.{bias, weight}[0m
[34mmask_decoder.output_upscaling.3.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.norm_final_attn.{bias, weight}[0m
[34mpe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.mask_downscaling.0.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.1.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.3.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.4.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.6.{bias, weight}[0m
[34mprompt_encoder.no_mask_embed.weight[0m
[34mprompt_encoder.not_a_point_embed.weight[0m
[34mprompt_encoder.pe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.point_embeddings.0.weight[0m
[34mprompt_encoder.point_embeddings.1.weight[0m
[34mprompt_encoder.point_embeddings.2.weight[0m
[34mprompt_encoder.point_embeddings.3.weight[0m
[34mproposal_generator.box_head.conv.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.4.{bias, weight}[0m
[34mproposal_generator.channel_attention.fc.0.weight[0m
[34mproposal_generator.channel_attention.fc.2.weight[0m
[34mproposal_generator.cls_head.conv.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p2_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p3_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p4_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.1.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.1.{bias, weight}[0m
[34mproposal_generator.pe_layer.positional_encoding_gaussian_matrix[0m
[34mproposal_generator.prompt_head.conv.0.{bias, weight}[0m
[34mproposal_generator.prompt_head.conv.1.{bias, running_mean, running_var, weight}[0m
[34mproposal_generator.prompt_head.mlp.1.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.3.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.5.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.objectness_logits.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.0.norm1.{bias, weight}[0m
[34mtransformer.layers.0.norm2.{bias, weight}[0m
[34mtransformer.layers.0.norm3.{bias, weight}[0m
[34mtransformer.layers.0.norm4.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.1.norm1.{bias, weight}[0m
[34mtransformer.layers.1.norm2.{bias, weight}[0m
[34mtransformer.layers.1.norm3.{bias, weight}[0m
[34mtransformer.layers.1.norm4.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.norm_final_attn.{bias, weight}[0m
[02/06 20:24:01] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.{bias, weight}[0m
  [35mfc1000.{bias, weight}[0m
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.shortcut.weight[0m
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv1.weight[0m
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv2.weight[0m
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv3.weight[0m
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv1.weight[0m
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv2.weight[0m
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv3.weight[0m
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv1.weight[0m
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv2.weight[0m
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv3.weight[0m
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.shortcut.weight[0m
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv1.weight[0m
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv2.weight[0m
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv3.weight[0m
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv1.weight[0m
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv2.weight[0m
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv3.weight[0m
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv1.weight[0m
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv2.weight[0m
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv3.weight[0m
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv1.weight[0m
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv2.weight[0m
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv3.weight[0m
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.shortcut.weight[0m
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv1.weight[0m
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv2.weight[0m
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv3.weight[0m
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv1.weight[0m
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv2.weight[0m
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv3.weight[0m
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv1.weight[0m
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv2.weight[0m
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv3.weight[0m
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv1.weight[0m
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv2.weight[0m
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv3.weight[0m
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv1.weight[0m
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv2.weight[0m
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv3.weight[0m
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv1.weight[0m
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv2.weight[0m
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv3.weight[0m
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.shortcut.weight[0m
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv1.weight[0m
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv2.weight[0m
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv3.weight[0m
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv1.weight[0m
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv2.weight[0m
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv3.weight[0m
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv1.weight[0m
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv2.weight[0m
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv3.weight[0m
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}[0m
[02/06 20:24:01] d2.engine.train_loop INFO: Starting training from iteration 0
[02/06 20:24:11] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 152, in train
    self.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 307, in run_step
    loss_dict = self.model(data)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/uwsam.py", line 197, in forward
    loss_cls_img = self.loss_cls(p_logits, gt_classes_target)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1362, in forward
    return F.cross_entropy(
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/functional.py", line 3504, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[02/06 20:24:11] d2.engine.hooks INFO: Total training time: 0:00:10 (0:00:00 on hooks)
[02/06 20:24:11] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 16302M
[02/06 20:25:20] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 20:25:21] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 20:25:21] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 20:25:21] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 20:25:21] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 20:25:21] d2.utils.env INFO: Using a generated random seed 23270511
[02/06 20:26:01] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 20:26:01] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 20:26:10] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 8.87 seconds.
[02/06 20:26:10] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 20:26:57] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 20:26:58] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 20:26:58] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 20:26:58] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 20:26:58] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 20:26:58] d2.utils.env INFO: Using a generated random seed 60332985
[02/06 20:28:46] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 20:28:48] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 20:28:48] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 20:28:48] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 20:28:48] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 20:28:48] d2.utils.env INFO: Using a generated random seed 50316773
[02/06 20:29:43] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 20:29:43] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 20:29:43] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 20:29:43] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 20:29:44] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 20:29:44] d2.utils.env INFO: Using a generated random seed 45792181
[02/06 20:30:23] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 20:30:23] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 20:30:31] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 7.95 seconds.
[02/06 20:30:31] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 20:30:34] d2.data.build INFO: Removed 0 images with no usable annotations. 8038 images left.
[02/06 20:30:35] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    fish    | 17238        |  reptiles  | 290          | arthropoda | 334          |
|   corals   | 9245         |  mollusk   | 1832         |   plants   | 1398         |
|   ruins    | 670          |  garbage   | 630          |   human    | 1511         |
|   robots   | 414          |            |              |            |              |
|   total    | 33562        |            |              |            |              |[0m
[02/06 20:30:35] d2.data.build INFO: Using training sampler TrainingSampler
[02/06 20:30:35] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/06 20:30:35] d2.data.common INFO: Serializing 8038 elements to byte tensors and concatenating them all ...
[02/06 20:30:37] d2.data.common INFO: Serialized dataset takes 138.37 MiB
[02/06 20:30:37] d2.data.build INFO: Making batched data loader with batch_size=4
[02/06 20:30:37] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:30:37] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/hoangnv/.torch/iopath_cache/detectron2/ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:30:38] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/06 20:30:38] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[02/06 20:30:38] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.vit.blocks.0.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm2.{bias, weight}[0m
[34mbackbone.vit.neck.0.weight[0m
[34mbackbone.vit.neck.1.{bias, weight}[0m
[34mbackbone.vit.neck.2.weight[0m
[34mbackbone.vit.neck.3.{bias, weight}[0m
[34mbackbone.vit.patch_embed.proj.{bias, weight}[0m
[34mbackbone.vit.pos_embed[0m
[34mmask_decoder.iou_prediction_head.layers.0.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.1.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.2.{bias, weight}[0m
[34mmask_decoder.iou_token.weight[0m
[34mmask_decoder.mask_tokens.weight[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.2.{bias, weight}[0m
[34mmask_decoder.output_upscaling.0.{bias, weight}[0m
[34mmask_decoder.output_upscaling.1.{bias, weight}[0m
[34mmask_decoder.output_upscaling.3.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.norm_final_attn.{bias, weight}[0m
[34mpe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.mask_downscaling.0.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.1.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.3.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.4.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.6.{bias, weight}[0m
[34mprompt_encoder.no_mask_embed.weight[0m
[34mprompt_encoder.not_a_point_embed.weight[0m
[34mprompt_encoder.pe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.point_embeddings.0.weight[0m
[34mprompt_encoder.point_embeddings.1.weight[0m
[34mprompt_encoder.point_embeddings.2.weight[0m
[34mprompt_encoder.point_embeddings.3.weight[0m
[34mproposal_generator.box_head.conv.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.4.{bias, weight}[0m
[34mproposal_generator.channel_attention.fc.0.weight[0m
[34mproposal_generator.channel_attention.fc.2.weight[0m
[34mproposal_generator.cls_head.conv.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p2_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p3_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p4_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.1.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.1.{bias, weight}[0m
[34mproposal_generator.pe_layer.positional_encoding_gaussian_matrix[0m
[34mproposal_generator.prompt_head.conv.0.{bias, weight}[0m
[34mproposal_generator.prompt_head.conv.1.{bias, running_mean, running_var, weight}[0m
[34mproposal_generator.prompt_head.mlp.1.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.3.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.5.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.objectness_logits.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.0.norm1.{bias, weight}[0m
[34mtransformer.layers.0.norm2.{bias, weight}[0m
[34mtransformer.layers.0.norm3.{bias, weight}[0m
[34mtransformer.layers.0.norm4.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.1.norm1.{bias, weight}[0m
[34mtransformer.layers.1.norm2.{bias, weight}[0m
[34mtransformer.layers.1.norm3.{bias, weight}[0m
[34mtransformer.layers.1.norm4.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.norm_final_attn.{bias, weight}[0m
[02/06 20:30:38] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.{bias, weight}[0m
  [35mfc1000.{bias, weight}[0m
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.shortcut.weight[0m
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv1.weight[0m
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv2.weight[0m
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv3.weight[0m
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv1.weight[0m
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv2.weight[0m
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv3.weight[0m
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv1.weight[0m
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv2.weight[0m
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv3.weight[0m
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.shortcut.weight[0m
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv1.weight[0m
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv2.weight[0m
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv3.weight[0m
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv1.weight[0m
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv2.weight[0m
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv3.weight[0m
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv1.weight[0m
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv2.weight[0m
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv3.weight[0m
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv1.weight[0m
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv2.weight[0m
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv3.weight[0m
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.shortcut.weight[0m
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv1.weight[0m
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv2.weight[0m
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv3.weight[0m
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv1.weight[0m
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv2.weight[0m
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv3.weight[0m
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv1.weight[0m
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv2.weight[0m
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv3.weight[0m
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv1.weight[0m
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv2.weight[0m
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv3.weight[0m
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv1.weight[0m
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv2.weight[0m
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv3.weight[0m
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv1.weight[0m
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv2.weight[0m
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv3.weight[0m
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.shortcut.weight[0m
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv1.weight[0m
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv2.weight[0m
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv3.weight[0m
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv1.weight[0m
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv2.weight[0m
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv3.weight[0m
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv1.weight[0m
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv2.weight[0m
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv3.weight[0m
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}[0m
[02/06 20:30:38] d2.engine.train_loop INFO: Starting training from iteration 0
[02/06 20:30:43] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 152, in train
    self.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 307, in run_step
    loss_dict = self.model(data)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/uwsam.py", line 190, in forward
    if target_labels.max() > self.num_classes - 1:
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[02/06 20:30:43] d2.engine.hooks INFO: Total training time: 0:00:05 (0:00:00 on hooks)
[02/06 20:30:43] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 16302M
[02/06 20:32:04] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 20:32:05] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 20:32:05] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 20:32:05] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 20:32:05] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 20:32:05] d2.utils.env INFO: Using a generated random seed 7286321
[02/06 20:32:43] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 20:32:43] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 20:32:52] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 8.32 seconds.
[02/06 20:32:52] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 20:32:55] d2.data.build INFO: Removed 0 images with no usable annotations. 8038 images left.
[02/06 20:32:56] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    fish    | 17238        |  reptiles  | 290          | arthropoda | 334          |
|   corals   | 9245         |  mollusk   | 1832         |   plants   | 1398         |
|   ruins    | 670          |  garbage   | 630          |   human    | 1511         |
|   robots   | 414          |            |              |            |              |
|   total    | 33562        |            |              |            |              |[0m
[02/06 20:32:56] d2.data.build INFO: Using training sampler TrainingSampler
[02/06 20:32:56] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/06 20:32:56] d2.data.common INFO: Serializing 8038 elements to byte tensors and concatenating them all ...
[02/06 20:32:59] d2.data.common INFO: Serialized dataset takes 138.37 MiB
[02/06 20:32:59] d2.data.build INFO: Making batched data loader with batch_size=4
[02/06 20:33:00] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:33:00] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/hoangnv/.torch/iopath_cache/detectron2/ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:33:00] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/06 20:33:01] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[02/06 20:33:01] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.vit.blocks.0.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm2.{bias, weight}[0m
[34mbackbone.vit.neck.0.weight[0m
[34mbackbone.vit.neck.1.{bias, weight}[0m
[34mbackbone.vit.neck.2.weight[0m
[34mbackbone.vit.neck.3.{bias, weight}[0m
[34mbackbone.vit.patch_embed.proj.{bias, weight}[0m
[34mbackbone.vit.pos_embed[0m
[34mmask_decoder.iou_prediction_head.layers.0.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.1.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.2.{bias, weight}[0m
[34mmask_decoder.iou_token.weight[0m
[34mmask_decoder.mask_tokens.weight[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.2.{bias, weight}[0m
[34mmask_decoder.output_upscaling.0.{bias, weight}[0m
[34mmask_decoder.output_upscaling.1.{bias, weight}[0m
[34mmask_decoder.output_upscaling.3.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.norm_final_attn.{bias, weight}[0m
[34mpe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.mask_downscaling.0.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.1.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.3.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.4.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.6.{bias, weight}[0m
[34mprompt_encoder.no_mask_embed.weight[0m
[34mprompt_encoder.not_a_point_embed.weight[0m
[34mprompt_encoder.pe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.point_embeddings.0.weight[0m
[34mprompt_encoder.point_embeddings.1.weight[0m
[34mprompt_encoder.point_embeddings.2.weight[0m
[34mprompt_encoder.point_embeddings.3.weight[0m
[34mproposal_generator.box_head.conv.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.4.{bias, weight}[0m
[34mproposal_generator.channel_attention.fc.0.weight[0m
[34mproposal_generator.channel_attention.fc.2.weight[0m
[34mproposal_generator.cls_head.conv.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p2_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p3_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p4_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.1.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.1.{bias, weight}[0m
[34mproposal_generator.pe_layer.positional_encoding_gaussian_matrix[0m
[34mproposal_generator.prompt_head.conv.0.{bias, weight}[0m
[34mproposal_generator.prompt_head.conv.1.{bias, running_mean, running_var, weight}[0m
[34mproposal_generator.prompt_head.mlp.1.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.3.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.5.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.objectness_logits.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.0.norm1.{bias, weight}[0m
[34mtransformer.layers.0.norm2.{bias, weight}[0m
[34mtransformer.layers.0.norm3.{bias, weight}[0m
[34mtransformer.layers.0.norm4.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.1.norm1.{bias, weight}[0m
[34mtransformer.layers.1.norm2.{bias, weight}[0m
[34mtransformer.layers.1.norm3.{bias, weight}[0m
[34mtransformer.layers.1.norm4.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.norm_final_attn.{bias, weight}[0m
[02/06 20:33:01] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.{bias, weight}[0m
  [35mfc1000.{bias, weight}[0m
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.shortcut.weight[0m
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv1.weight[0m
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv2.weight[0m
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv3.weight[0m
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv1.weight[0m
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv2.weight[0m
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv3.weight[0m
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv1.weight[0m
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv2.weight[0m
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv3.weight[0m
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.shortcut.weight[0m
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv1.weight[0m
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv2.weight[0m
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv3.weight[0m
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv1.weight[0m
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv2.weight[0m
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv3.weight[0m
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv1.weight[0m
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv2.weight[0m
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv3.weight[0m
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv1.weight[0m
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv2.weight[0m
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv3.weight[0m
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.shortcut.weight[0m
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv1.weight[0m
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv2.weight[0m
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv3.weight[0m
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv1.weight[0m
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv2.weight[0m
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv3.weight[0m
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv1.weight[0m
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv2.weight[0m
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv3.weight[0m
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv1.weight[0m
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv2.weight[0m
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv3.weight[0m
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv1.weight[0m
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv2.weight[0m
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv3.weight[0m
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv1.weight[0m
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv2.weight[0m
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv3.weight[0m
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.shortcut.weight[0m
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv1.weight[0m
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv2.weight[0m
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv3.weight[0m
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv1.weight[0m
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv2.weight[0m
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv3.weight[0m
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv1.weight[0m
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv2.weight[0m
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv3.weight[0m
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}[0m
[02/06 20:33:01] d2.engine.train_loop INFO: Starting training from iteration 0
[02/06 20:33:08] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 152, in train
    self.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 307, in run_step
    loss_dict = self.model(data)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/uwsam.py", line 196, in forward
    max_label = int(target_labels.max().item()) if target_labels.numel() > 0 else 0
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[02/06 20:33:08] d2.engine.hooks INFO: Total training time: 0:00:06 (0:00:00 on hooks)
[02/06 20:33:08] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 16302M
[02/06 20:34:46] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 20:34:47] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 20:34:47] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 20:34:47] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 20:34:47] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 20:34:47] d2.utils.env INFO: Using a generated random seed 49533385
[02/06 20:35:32] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 20:35:32] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 20:35:41] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 8.74 seconds.
[02/06 20:35:41] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 20:35:45] d2.data.build INFO: Removed 0 images with no usable annotations. 8038 images left.
[02/06 20:35:46] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    fish    | 17238        |  reptiles  | 290          | arthropoda | 334          |
|   corals   | 9245         |  mollusk   | 1832         |   plants   | 1398         |
|   ruins    | 670          |  garbage   | 630          |   human    | 1511         |
|   robots   | 414          |            |              |            |              |
|   total    | 33562        |            |              |            |              |[0m
[02/06 20:35:46] d2.data.build INFO: Using training sampler TrainingSampler
[02/06 20:35:46] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/06 20:35:46] d2.data.common INFO: Serializing 8038 elements to byte tensors and concatenating them all ...
[02/06 20:35:48] d2.data.common INFO: Serialized dataset takes 138.37 MiB
[02/06 20:35:48] d2.data.build INFO: Making batched data loader with batch_size=4
[02/06 20:35:49] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:35:49] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/hoangnv/.torch/iopath_cache/detectron2/ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:35:50] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/06 20:35:50] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[02/06 20:35:50] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.vit.blocks.0.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm2.{bias, weight}[0m
[34mbackbone.vit.neck.0.weight[0m
[34mbackbone.vit.neck.1.{bias, weight}[0m
[34mbackbone.vit.neck.2.weight[0m
[34mbackbone.vit.neck.3.{bias, weight}[0m
[34mbackbone.vit.patch_embed.proj.{bias, weight}[0m
[34mbackbone.vit.pos_embed[0m
[34mmask_decoder.iou_prediction_head.layers.0.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.1.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.2.{bias, weight}[0m
[34mmask_decoder.iou_token.weight[0m
[34mmask_decoder.mask_tokens.weight[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.2.{bias, weight}[0m
[34mmask_decoder.output_upscaling.0.{bias, weight}[0m
[34mmask_decoder.output_upscaling.1.{bias, weight}[0m
[34mmask_decoder.output_upscaling.3.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.norm_final_attn.{bias, weight}[0m
[34mpe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.mask_downscaling.0.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.1.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.3.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.4.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.6.{bias, weight}[0m
[34mprompt_encoder.no_mask_embed.weight[0m
[34mprompt_encoder.not_a_point_embed.weight[0m
[34mprompt_encoder.pe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.point_embeddings.0.weight[0m
[34mprompt_encoder.point_embeddings.1.weight[0m
[34mprompt_encoder.point_embeddings.2.weight[0m
[34mprompt_encoder.point_embeddings.3.weight[0m
[34mproposal_generator.box_head.conv.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.4.{bias, weight}[0m
[34mproposal_generator.channel_attention.fc.0.weight[0m
[34mproposal_generator.channel_attention.fc.2.weight[0m
[34mproposal_generator.cls_head.conv.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p2_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p3_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p4_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.1.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.1.{bias, weight}[0m
[34mproposal_generator.pe_layer.positional_encoding_gaussian_matrix[0m
[34mproposal_generator.prompt_head.conv.0.{bias, weight}[0m
[34mproposal_generator.prompt_head.conv.1.{bias, running_mean, running_var, weight}[0m
[34mproposal_generator.prompt_head.mlp.1.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.3.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.5.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.objectness_logits.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.0.norm1.{bias, weight}[0m
[34mtransformer.layers.0.norm2.{bias, weight}[0m
[34mtransformer.layers.0.norm3.{bias, weight}[0m
[34mtransformer.layers.0.norm4.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.1.norm1.{bias, weight}[0m
[34mtransformer.layers.1.norm2.{bias, weight}[0m
[34mtransformer.layers.1.norm3.{bias, weight}[0m
[34mtransformer.layers.1.norm4.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.norm_final_attn.{bias, weight}[0m
[02/06 20:35:50] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.{bias, weight}[0m
  [35mfc1000.{bias, weight}[0m
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.shortcut.weight[0m
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv1.weight[0m
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv2.weight[0m
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv3.weight[0m
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv1.weight[0m
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv2.weight[0m
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv3.weight[0m
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv1.weight[0m
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv2.weight[0m
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv3.weight[0m
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.shortcut.weight[0m
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv1.weight[0m
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv2.weight[0m
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv3.weight[0m
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv1.weight[0m
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv2.weight[0m
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv3.weight[0m
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv1.weight[0m
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv2.weight[0m
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv3.weight[0m
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv1.weight[0m
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv2.weight[0m
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv3.weight[0m
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.shortcut.weight[0m
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv1.weight[0m
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv2.weight[0m
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv3.weight[0m
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv1.weight[0m
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv2.weight[0m
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv3.weight[0m
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv1.weight[0m
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv2.weight[0m
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv3.weight[0m
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv1.weight[0m
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv2.weight[0m
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv3.weight[0m
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv1.weight[0m
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv2.weight[0m
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv3.weight[0m
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv1.weight[0m
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv2.weight[0m
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv3.weight[0m
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.shortcut.weight[0m
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv1.weight[0m
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv2.weight[0m
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv3.weight[0m
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv1.weight[0m
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv2.weight[0m
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv3.weight[0m
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv1.weight[0m
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv2.weight[0m
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv3.weight[0m
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}[0m
[02/06 20:35:50] d2.engine.train_loop INFO: Starting training from iteration 0
[02/06 20:35:56] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 152, in train
    self.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/hoangnv/detectron2/detectron2/engine/train_loop.py", line 307, in run_step
    loss_dict = self.model(data)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hoangnv/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hoangnv/UWSAM/core/uwsam.py", line 199, in forward
    target_labels_cpu = target_labels.cpu()
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[02/06 20:35:56] d2.engine.hooks INFO: Total training time: 0:00:06 (0:00:00 on hooks)
[02/06 20:35:56] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 16302M
[02/06 20:37:53] detectron2 INFO: Rank of current process: 0. World size: 1
[02/06 20:37:55] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
numpy                            2.2.6
detectron2                       0.6 @/home/hoangnv/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.6
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.10.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  True
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   565.77
CUDA_HOME                        /usr
Pillow                           12.1.0
torchvision                      0.25.0+cu128 @/home/hoangnv/.venv/lib/python3.10/site-packages/torchvision
torchvision arch flags           /home/hoangnv/.venv/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              Not found
-------------------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.8
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120
  - CuDNN 91.0.2  (built against CUDA 12.9)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=449b1768410104d3ed79d3bcfe4ba1d65c7f22c0, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_FBGEMM_GENAI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

[02/06 20:37:55] detectron2 INFO: Command line arguments: Namespace(config_file='', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50172', opts=[])
[02/06 20:37:55] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - uiis10k_test
  TRAIN:
  - uiis10k_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1024
  MAX_SIZE_TRAIN: 1024
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 1024
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
    - - 64
    - - 128
    - - 256
    - - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: SAMBackbone
  DEVICE: cuda
  EUPG:
    NMS_THRESH: 0.5
    NUM_PROPOSALS: 64
    SCORE_THRESH: 0.5
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: true
  META_ARCHITECTURE: UWSAM
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: FastRCNNConvFCHead
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.0
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 10
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    - p6
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SAM:
    CHECKPOINT: /home/hoangnv/MaskRCNN-LoRA-/weights/sam_vit_h_4b8939.pth
    FREEZE: true
    IMAGE_SIZE: 1024
    LORA:
      ALPHA: 8
      DROPOUT: 0.05
      ENABLED: true
      RANK: 8
    TYPE: vit_h
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl
OUTPUT_DIR: ./output/uwsam_vit_h_lora_standard_bs
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 2009
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 48216
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30135
  - 40180
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 48216
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[02/06 20:37:55] detectron2 INFO: Full config saved to ./output/uwsam_vit_h_lora_standard_bs/config.yaml
[02/06 20:37:55] d2.utils.env INFO: Using a generated random seed 56815945
[02/06 20:38:32] d2.engine.defaults INFO: Model:
UWSAM(
  (backbone): SAMBackbone(
    (vit): ImageEncoderViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0-31): 32 x CheckpointedBlock(
          (block): Block(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): LoRA_SAM_QKV(
                (qkv): Linear(in_features=1280, out_features=3840, bias=True)
                (lora_A_q): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_q): Linear(in_features=8, out_features=1280, bias=False)
                (lora_A_v): Linear(in_features=1280, out_features=8, bias=False)
                (lora_B_v): Linear(in_features=8, out_features=1280, bias=False)
                (dropout): Dropout(p=0.05, inplace=False)
              )
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): MLPBlock(
              (lin1): Linear(in_features=1280, out_features=5120, bias=True)
              (lin2): Linear(in_features=5120, out_features=1280, bias=True)
              (act): GELU(approximate='none')
            )
          )
        )
      )
      (neck): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm2d()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (3): LayerNorm2d()
      )
    )
  )
  (proposal_generator): EUPG(
    (channel_attention): ChannelAttention(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (max_pool): AdaptiveMaxPool2d(output_size=1)
      (fc): Sequential(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (sigmoid): Sigmoid()
    )
    (multiscale_gen): MultiScaleGenerator(
      (up_p3): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (up_p2): Sequential(
        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
      )
      (p2_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p3_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (p4_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (pe_layer): PositionEmbeddingRandom()
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    (rpn): RPN(
      (rpn_head): StandardRPNHead(
        (conv): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
        (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
      )
      (anchor_generator): DefaultAnchorGenerator(
        (cell_anchors): BufferList()
      )
    )
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (prompt_head): PromptEncoder(
      (conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (mlp): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=12544, out_features=256, bias=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): ReLU(inplace=True)
        (5): Linear(in_features=256, out_features=2560, bias=True)
      )
    )
    (cls_head): ClassificationHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1024, out_features=11, bias=True)
      )
    )
    (box_head): BoxRegressionHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mlp): Sequential(
        (0): Linear(in_features=50176, out_features=1024, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=1024, out_features=1024, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
  (loss_cls): CrossEntropyLoss()
  (transformer): TwoWayTransformer(
    (layers): ModuleList(
      (0-1): 2 x TwoWayAttentionBlock(
        (self_attn): Attention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_token_to_image): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (act): ReLU()
        )
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (cross_attn_image_to_token): Attention(
          (q_proj): Linear(in_features=256, out_features=128, bias=True)
          (k_proj): Linear(in_features=256, out_features=128, bias=True)
          (v_proj): Linear(in_features=256, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (final_attn_token_to_image): Attention(
      (q_proj): Linear(in_features=256, out_features=128, bias=True)
      (k_proj): Linear(in_features=256, out_features=128, bias=True)
      (v_proj): Linear(in_features=256, out_features=128, bias=True)
      (out_proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (pe_layer): PositionEmbeddingRandom()
  (criterion): UWSAMCriterion()
)
[02/06 20:38:32] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024), pad_value=128.0), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomContrast(intensity_min=0.9, intensity_max=1.1)]
[02/06 20:38:40] d2.data.datasets.coco INFO: Loading /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json takes 8.01 seconds.
[02/06 20:38:40] d2.data.datasets.coco INFO: Loaded 8038 images in COCO format from /data/bailab_data/hoangnv/UIIS10K/annotations/multiclass_train.json
[02/06 20:38:43] d2.data.build INFO: Removed 0 images with no usable annotations. 8038 images left.
[02/06 20:38:44] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|    fish    | 17238        |  reptiles  | 290          | arthropoda | 334          |
|   corals   | 9245         |  mollusk   | 1832         |   plants   | 1398         |
|   ruins    | 670          |  garbage   | 630          |   human    | 1511         |
|   robots   | 414          |            |              |            |              |
|   total    | 33562        |            |              |            |              |[0m
[02/06 20:38:44] d2.data.build INFO: Using training sampler TrainingSampler
[02/06 20:38:44] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/06 20:38:44] d2.data.common INFO: Serializing 8038 elements to byte tensors and concatenating them all ...
[02/06 20:38:45] d2.data.common INFO: Serialized dataset takes 138.37 MiB
[02/06 20:38:45] d2.data.build INFO: Making batched data loader with batch_size=4
[02/06 20:38:46] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:38:46] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/hoangnv/.torch/iopath_cache/detectron2/ImageNetPretrained/MSRA/R-50.pkl ...
[02/06 20:38:46] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/06 20:38:46] d2.checkpoint.c2_model_loading WARNING: No weights in checkpoint matched with model.
[02/06 20:38:47] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.vit.blocks.0.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.0.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.0.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.1.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.1.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.10.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.10.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.11.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.11.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.12.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.12.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.13.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.13.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.14.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.14.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.15.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.15.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.16.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.16.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.17.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.17.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.18.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.18.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.19.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.19.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.2.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.2.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.20.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.20.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.21.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.21.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.22.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.22.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.23.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.23.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.24.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.24.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.25.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.25.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.26.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.26.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.27.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.27.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.28.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.28.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.29.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.29.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.3.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.3.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.30.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.30.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.31.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.31.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.4.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.4.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.5.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.5.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.6.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.6.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.7.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.7.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.8.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.8.block.norm2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.proj.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_A_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_q.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.lora_B_v.weight[0m
[34mbackbone.vit.blocks.9.block.attn.qkv.qkv.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.attn.{rel_pos_h, rel_pos_w}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.mlp.lin2.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm1.{bias, weight}[0m
[34mbackbone.vit.blocks.9.block.norm2.{bias, weight}[0m
[34mbackbone.vit.neck.0.weight[0m
[34mbackbone.vit.neck.1.{bias, weight}[0m
[34mbackbone.vit.neck.2.weight[0m
[34mbackbone.vit.neck.3.{bias, weight}[0m
[34mbackbone.vit.patch_embed.proj.{bias, weight}[0m
[34mbackbone.vit.pos_embed[0m
[34mmask_decoder.iou_prediction_head.layers.0.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.1.{bias, weight}[0m
[34mmask_decoder.iou_prediction_head.layers.2.{bias, weight}[0m
[34mmask_decoder.iou_token.weight[0m
[34mmask_decoder.mask_tokens.weight[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.0.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.1.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.2.layers.2.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.0.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.1.{bias, weight}[0m
[34mmask_decoder.output_hypernetworks_mlps.3.layers.2.{bias, weight}[0m
[34mmask_decoder.output_upscaling.0.{bias, weight}[0m
[34mmask_decoder.output_upscaling.1.{bias, weight}[0m
[34mmask_decoder.output_upscaling.3.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm1.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm2.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm3.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.norm4.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mmask_decoder.transformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mmask_decoder.transformer.norm_final_attn.{bias, weight}[0m
[34mpe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.mask_downscaling.0.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.1.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.3.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.4.{bias, weight}[0m
[34mprompt_encoder.mask_downscaling.6.{bias, weight}[0m
[34mprompt_encoder.no_mask_embed.weight[0m
[34mprompt_encoder.not_a_point_embed.weight[0m
[34mprompt_encoder.pe_layer.positional_encoding_gaussian_matrix[0m
[34mprompt_encoder.point_embeddings.0.weight[0m
[34mprompt_encoder.point_embeddings.1.weight[0m
[34mprompt_encoder.point_embeddings.2.weight[0m
[34mprompt_encoder.point_embeddings.3.weight[0m
[34mproposal_generator.box_head.conv.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.box_head.mlp.4.{bias, weight}[0m
[34mproposal_generator.channel_attention.fc.0.weight[0m
[34mproposal_generator.channel_attention.fc.2.weight[0m
[34mproposal_generator.cls_head.conv.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.0.{bias, weight}[0m
[34mproposal_generator.cls_head.mlp.2.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p2_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p3_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.p4_out.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p2.1.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.0.{bias, weight}[0m
[34mproposal_generator.multiscale_gen.up_p3.1.{bias, weight}[0m
[34mproposal_generator.pe_layer.positional_encoding_gaussian_matrix[0m
[34mproposal_generator.prompt_head.conv.0.{bias, weight}[0m
[34mproposal_generator.prompt_head.conv.1.{bias, running_mean, running_var, weight}[0m
[34mproposal_generator.prompt_head.mlp.1.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.3.{bias, weight}[0m
[34mproposal_generator.prompt_head.mlp.5.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn.rpn_head.objectness_logits.{bias, weight}[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.final_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.0.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.0.norm1.{bias, weight}[0m
[34mtransformer.layers.0.norm2.{bias, weight}[0m
[34mtransformer.layers.0.norm3.{bias, weight}[0m
[34mtransformer.layers.0.norm4.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.0.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_image_to_token.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.cross_attn_token_to_image.v_proj.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin1.{bias, weight}[0m
[34mtransformer.layers.1.mlp.lin2.{bias, weight}[0m
[34mtransformer.layers.1.norm1.{bias, weight}[0m
[34mtransformer.layers.1.norm2.{bias, weight}[0m
[34mtransformer.layers.1.norm3.{bias, weight}[0m
[34mtransformer.layers.1.norm4.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.k_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.q_proj.{bias, weight}[0m
[34mtransformer.layers.1.self_attn.v_proj.{bias, weight}[0m
[34mtransformer.norm_final_attn.{bias, weight}[0m
[02/06 20:38:47] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.{bias, weight}[0m
  [35mfc1000.{bias, weight}[0m
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.shortcut.weight[0m
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv1.weight[0m
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv2.weight[0m
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.0.conv3.weight[0m
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv1.weight[0m
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv2.weight[0m
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.1.conv3.weight[0m
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv1.weight[0m
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv2.weight[0m
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres2.2.conv3.weight[0m
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.shortcut.weight[0m
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv1.weight[0m
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv2.weight[0m
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.0.conv3.weight[0m
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv1.weight[0m
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv2.weight[0m
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.1.conv3.weight[0m
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv1.weight[0m
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv2.weight[0m
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.2.conv3.weight[0m
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv1.weight[0m
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv2.weight[0m
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres3.3.conv3.weight[0m
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.shortcut.weight[0m
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv1.weight[0m
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv2.weight[0m
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.0.conv3.weight[0m
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv1.weight[0m
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv2.weight[0m
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.1.conv3.weight[0m
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv1.weight[0m
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv2.weight[0m
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.2.conv3.weight[0m
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv1.weight[0m
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv2.weight[0m
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.3.conv3.weight[0m
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv1.weight[0m
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv2.weight[0m
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.4.conv3.weight[0m
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv1.weight[0m
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv2.weight[0m
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres4.5.conv3.weight[0m
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.shortcut.weight[0m
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv1.weight[0m
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv2.weight[0m
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.0.conv3.weight[0m
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv1.weight[0m
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv2.weight[0m
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.1.conv3.weight[0m
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv1.weight[0m
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv2.weight[0m
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mres5.2.conv3.weight[0m
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}[0m
[02/06 20:38:47] d2.engine.train_loop INFO: Starting training from iteration 0
[02/06 20:39:21] d2.utils.events INFO:  eta: 18:39:33  iter: 19  total_loss: 2.289  loss_rpn_cls: 0.6961  loss_rpn_loc: 0.6053  loss_cls: 1.033  loss_box_reg: 0  loss_mask: 0    time: 1.4073  last_time: 1.3555  data_time: 0.1366  last_data_time: 0.0289   lr: 3.9962e-06  max_mem: 16949M
[02/06 20:39:50] d2.utils.events INFO:  eta: 18:46:55  iter: 39  total_loss: 1.322  loss_rpn_cls: 0.6907  loss_rpn_loc: 0.6031  loss_cls: 0.01253  loss_box_reg: 0  loss_mask: 0    time: 1.4271  last_time: 1.5010  data_time: 0.0449  last_data_time: 0.0494   lr: 7.9922e-06  max_mem: 16949M
[02/06 20:40:21] d2.utils.events INFO:  eta: 19:26:11  iter: 59  total_loss: 1.401  loss_rpn_cls: 0.6776  loss_rpn_loc: 0.7164  loss_cls: 0.004778  loss_box_reg: 0  loss_mask: 0    time: 1.4671  last_time: 1.6103  data_time: 0.0619  last_data_time: 0.1005   lr: 1.1988e-05  max_mem: 16949M
[02/06 20:40:52] d2.utils.events INFO:  eta: 19:45:10  iter: 79  total_loss: 1.305  loss_rpn_cls: 0.6571  loss_rpn_loc: 0.57  loss_cls: 0.002827  loss_box_reg: 0  loss_mask: 0    time: 1.4910  last_time: 1.4936  data_time: 0.0701  last_data_time: 0.0520   lr: 1.5984e-05  max_mem: 16949M
[02/06 20:41:22] d2.utils.events INFO:  eta: 19:44:40  iter: 99  total_loss: 1.293  loss_rpn_cls: 0.6088  loss_rpn_loc: 0.6011  loss_cls: 0.0117  loss_box_reg: 0  loss_mask: 0    time: 1.4921  last_time: 1.4885  data_time: 0.0472  last_data_time: 0.0621   lr: 1.998e-05  max_mem: 16949M
[02/06 20:41:51] d2.utils.events INFO:  eta: 19:40:04  iter: 119  total_loss: 1.214  loss_rpn_cls: 0.533  loss_rpn_loc: 0.6896  loss_cls: 0.0005821  loss_box_reg: 0  loss_mask: 0    time: 1.4837  last_time: 1.3790  data_time: 0.0469  last_data_time: 0.0581   lr: 2.3976e-05  max_mem: 16949M
[02/06 20:42:02] d2.engine.hooks INFO: Overall training speed: 125 iterations in 0:03:06 (1.4913 s / it)
[02/06 20:42:02] d2.engine.hooks INFO: Total training time: 0:03:06 (0:00:00 on hooks)
[02/06 20:42:02] d2.utils.events INFO:  eta: 19:38:31  iter: 127  total_loss: 1.065  loss_rpn_cls: 0.4907  loss_rpn_loc: 0.5902  loss_cls: 0.0003178  loss_box_reg: 0  loss_mask: 0    time: 1.4803  last_time: 1.3702  data_time: 0.0424  last_data_time: 0.0412   lr: 2.5375e-05  max_mem: 16949M
